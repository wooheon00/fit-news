package com.fitnews.fit_news.news.service;

import com.fitnews.fit_news.news.entity.News;
import com.fitnews.fit_news.news.model.NewsData;
import com.fitnews.fit_news.news.repository.NewsRepository;
import com.fitnews.fit_news.news.repository.NewsTendencyRepository;
import lombok.RequiredArgsConstructor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.boot.context.event.ApplicationReadyEvent;
import org.springframework.context.event.EventListener;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Component;
import java.util.Comparator;
import java.util.Map;
import java.util.stream.Collectors;

import java.net.URI;
import java.time.LocalDateTime;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

@Component
@RequiredArgsConstructor
public class NewsScheduler {
    private static final Logger logger = LoggerFactory.getLogger(NewsScheduler.class);

    // í•œ ë²ˆì— 5ê°œì”© ë¶„ë¥˜
    private static final int BATCH_SIZE = 5;

    private final NewsCrawlingService newsCrawlingService;
    private final OpenAIAPIService openAIAPIService;
    private final NewsClassificationService newsClassificationService;
    private final NewsTendencyService newsTendencyService;
    private final NewsService newsService;

    // ì¤‘ë³µ(ì´ë¯¸ ì €ì¥ëœ ë‰´ìŠ¤/ì„±í–¥) ì²´í¬ìš©
    private final NewsRepository newsRepository;
    private final NewsTendencyRepository newsTendencyRepository;

    /**
     *  âœ… ì„œë²„ ì‹œì‘ í›„ 1ë²ˆ ì¦‰ì‹œ ì‹¤í–‰
     */
    @EventListener(ApplicationReadyEvent.class)
    public void runOnStartup() {
        logger.info("Application started. Running initial crawling job.");
        runCrawlingJob();
    }


    @Scheduled(fixedRate = 30 * 60 * 1000L)
    public void runCrawlingJob() {
        long crawlingTime = System.currentTimeMillis();
        logger.info("NewsScheduler triggered at {}", crawlingTime);

        try {
            // 1) í¬ë¡¤ë§
            List<NewsData> rawNews = newsCrawlingService.crawlingNews(crawlingTime);
            if (rawNews == null || rawNews.isEmpty()) {
                logger.info("No new news items found at {}", crawlingTime);
                return;
            }
            logger.info("âœ… 1 Crawling Clear: {} items", rawNews.size());

            // 2) ë§í¬ ì •ê·œí™” + ë°°ì¹˜ë‚´ ì¤‘ë³µ ì œê±°
            List<NewsData> deDuped = deDuplicateInBatch(rawNews);
            logger.info("âœ… De-dup in batch: {} -> {}", rawNews.size(), deDuped.size());

            // 3) ì–¸ë¡ ì‚¬ë³„ë¡œ ê·¸ë£¹í•‘ + DB ê¸°ì¤€ ì¤‘ë³µ ì œê±° + ì–¸ë¡ ì‚¬ë³„ ìµœì‹  10ê°œë§Œ
            Map<String, List<NewsData>> groupedBySource = deDuped.stream()
                    .collect(Collectors.groupingBy(nd ->
                            newsClassificationService.detectSourceFromLink(nd.getLink())
                    ));

            List<NewsData> onlyNew = groupedBySource.values().stream()
                    .flatMap(listForSource ->
                            listForSource.stream()
                                    // DBì— ì´ë¯¸ ìˆëŠ” ë§í¬ëŠ” ì œì™¸
                                    .filter(nd -> !newsRepository.existsByLink(nd.getLink()))
                                    // pubDate ê¸°ì¤€ ìµœì‹ ìˆœ ì •ë ¬
                                    .sorted(Comparator.comparing(
                                            NewsData::getPubDate,
                                            Comparator.nullsLast(Comparator.naturalOrder())
                                    ).reversed())
                                    // ğŸ”¥ ì–¸ë¡ ì‚¬ë³„ ìƒìœ„ 10ê°œë§Œ
                                    .limit(10)
                    )
                    .toList();

            if (onlyNew.isEmpty()) {
                logger.info("All crawled items already exist in DB (by source). Nothing to analyze/save.");
                return;
            }
            logger.info("âœ… After DB de-dup & per-source limit: {} items to analyze", onlyNew.size());

            // 4) ë°°ì¹˜ ë¶„ë¥˜/ì €ì¥
            int totalSavedNews = 0;
            int totalSavedTendency = 0;
            int totalSkippedNews = 0;

            for (int i = 0; i < onlyNew.size(); i += BATCH_SIZE) {
                List<NewsData> batch = onlyNew.subList(i, Math.min(i + BATCH_SIZE, onlyNew.size()));
                int batchIdx = (i / BATCH_SIZE) + 1;

                logger.info("â¡ï¸ Batch {} start (size={})", batchIdx, batch.size());

                String prompt = newsClassificationService.preprocessingNews(batch);
                logger.info("âœ… 2 Preprocessing Success (batch={})", batchIdx);

                String response = openAIAPIService.askChatGPT(prompt);

                // ì—ëŸ¬/ë¹„ì •ìƒ ì‘ë‹µì´ë©´ ë¶„ë¥˜ ìŠ¤í‚µí•˜ê³  ë‰´ìŠ¤ë§Œ ì €ì¥
                if (checkingResponseValidForBatch(response, batch, batchIdx)) {
                    // ì´ë¯¸ ì €ì¥/ë¡œê¹… ë
                    continue;
                }
                logger.info("âœ… 3 Request Success (batch={})", batchIdx);

                // í›„ì²˜ë¦¬(ë¶„ë¥˜ ê²°ê³¼ ì£¼ì…)
                List<NewsData> classified = newsClassificationService.postprocessingNews(response, batch);
                logger.info("âœ… 4 PostProcessing Success (batch={})", batchIdx);

                // ì €ì¥: News â†’ (ìˆë‹¤ë©´) NewsTendency
                int savedNewsCnt = 0, savedTendencyCnt = 0, skippedNewsCnt = 0;
                for (NewsData nd : classified) {

                    // ğŸ”¥ 1) ì„±í–¥(Tc)ì´ ì—†ìœ¼ë©´ ì´ ë‰´ìŠ¤ëŠ” í†µì§¸ë¡œ ìŠ¤í‚µ
                    if (nd.getNewsTc() == null) {
                        skippedNewsCnt++;
                        logger.warn("âš ï¸ No NewsTendency for this news. Skip saving. title={}, link={}",
                                nd.getTitle(), nd.getLink());
                        continue;
                    }

                    // 2) DTO â†’ News ì €ì¥ (ì¤‘ë³µì€ ë‚´ë¶€ì—ì„œ ë°©ì§€)
                    News savedNews = newsService.saveNews(nd.toNewsEntity());
                    if (savedNews == null) {
                        skippedNewsCnt++;
                        continue;
                    }
                    savedNewsCnt++;

                    // 3) ì„±í–¥ ì €ì¥ (ì´ë¯¸ ì¡´ì¬í•˜ë©´ upsert)
                    newsTendencyService.saveOrUpdateFromTc(savedNews, nd.getNewsTc());
                    savedTendencyCnt++;
                }

                totalSavedNews += savedNewsCnt;
                totalSavedTendency += savedTendencyCnt;
                totalSkippedNews += skippedNewsCnt;

                logger.info("âœ… Batch {} saved: news={}, tendencies={}, skipped={}",
                        batchIdx, savedNewsCnt, savedTendencyCnt, skippedNewsCnt);
            }

            logger.info("ğŸ All batches done. totalSavedNews={}, totalSavedTendency={}, totalSkippedNews={}",
                    totalSavedNews, totalSavedTendency, totalSkippedNews);

        } catch (Exception e) {
            logger.error("Error occurred during scheduled crawling: ", e);
        }
    }

    /**
     * OpenAI ì—ëŸ¬/ë¹„ì •ìƒ ì‘ë‹µì´ë©´ ë¶„ë¥˜ ìŠ¤í‚µí•˜ê³  ë‰´ìŠ¤ë§Œ ì €ì¥
     */
    private boolean checkingResponseValidForBatch(String response, List<NewsData> batch, int batchIdx) {
        if (response == null
                || response.startsWith("OpenAI API Error")
                || response.startsWith("Unexpected error")) {

            logger.warn("âš ï¸ Skip whole batch (batch={}) due to OpenAI error/invalid response: {}",
                    batchIdx, response);

            // â›”ï¸ ë” ì´ìƒ ë‰´ìŠ¤ë„ ì €ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤
            return true;   // í˜¸ì¶œë¶€ì—ì„œ continue;
        }
        return false;
    }

    /**
     * ë§í¬ ì •ê·œí™” + ê°™ì€ ë°°ì¹˜ ë‚´ ì¤‘ë³µ ì œê±°
     */
    private List<NewsData> deDuplicateInBatch(List<NewsData> rawNews) {
        Set<String> seen = new HashSet<>();
        return rawNews.stream()
                .peek(nd -> nd.setLink(normalizeLink(nd.getLink())))
                .filter(nd -> seen.add(nd.getLink()))
                .toList();
    }

    /**
     * ì¿¼ë¦¬/í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°í•˜ì—¬ ë§í¬ í‘œì¤€í™”
     */
    private String normalizeLink(String link) {
        if (link == null) return null;
        try {
            URI uri = new URI(link);
            return new URI(
                    uri.getScheme(),
                    uri.getAuthority(),
                    uri.getPath(),
                    null,  // query ì œê±°
                    null   // fragment ì œê±°
            ).toString();
        } catch (Exception e) {
            return link; // ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
        }
    }
}
